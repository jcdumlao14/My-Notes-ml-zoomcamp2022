{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **5. Deploying Machine Learning Models**"
      ],
      "metadata": {
        "id": "-L1oZl8pxAXl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **5.1 Intro / Session overview** (video 1)\n",
        "\n",
        "## **Notes**\n",
        "____\n",
        "In This session, we talked about the earlier model we made in chapter 3 for churn prediction.\n",
        "\n",
        "This chapter contains the deployment of the model. If we want to use the model to predict new values without running the code, There's a way to do this. The way to use the model on different machines without running the code is to deploy the model on a server (run the code and make the model). After deploying the code in a machine used as server we can make some endpoints (using api's) to connect from another machine to the server and predict values.\n",
        "\n",
        "To deploy the model in a server there are some steps:\n",
        "\n",
        "* After training the model save it, to use it for making predictions in future (session 02-pickle).\n",
        "* Make the API endpoints in order to request predictions. (session 03-flask-intro and 04-flask-deployment)\n",
        "* Some other server deployment options (sessions 5 to 9)\n"
      ],
      "metadata": {
        "id": "aIGR_D_ZXwhy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the previous session we trained a model for predicting churn and evaluated it. Now let's deploy it"
      ],
      "metadata": {
        "id": "rwges51LXqzR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rth1om-BXomh"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import KFold\n",
        "\n",
        "from sklearn.feature_extraction import DictVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import roc_auc_score"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = 'https://raw.githubusercontent.com/alexeygrigorev/mlbookcamp-code/master/chapter-03-churn-prediction/WA_Fn-UseC_-Telco-Customer-Churn.csv'\n"
      ],
      "metadata": {
        "id": "aYqPYler0uIp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget $data -O data-week-3.csv "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pa0osRoT1ChE",
        "outputId": "26640d14-7aad-4dc7-d8cf-b83b8b434690"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-10-04 21:09:34--  https://raw.githubusercontent.com/alexeygrigorev/mlbookcamp-code/master/chapter-03-churn-prediction/WA_Fn-UseC_-Telco-Customer-Churn.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.110.133, 185.199.109.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 977501 (955K) [text/plain]\n",
            "Saving to: ‘data-week-3.csv’\n",
            "\n",
            "data-week-3.csv     100%[===================>] 954.59K  --.-KB/s    in 0.1s    \n",
            "\n",
            "2022-10-04 21:09:36 (8.45 MB/s) - ‘data-week-3.csv’ saved [977501/977501]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('data-week-3.csv')\n",
        "\n",
        "df.columns = df.columns.str.lower().str.replace(' ', '_')\n",
        "\n",
        "categorical_columns = list(df.dtypes[df.dtypes == 'object'].index)\n",
        "\n",
        "for c in categorical_columns:\n",
        "    df[c] = df[c].str.lower().str.replace(' ', '_')\n",
        "\n",
        "df.totalcharges = pd.to_numeric(df.totalcharges, errors='coerce')\n",
        "df.totalcharges = df.totalcharges.fillna(0)\n",
        "\n",
        "df.churn = (df.churn == 'yes').astype(int)"
      ],
      "metadata": {
        "id": "asb1CYymX19V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_full_train, df_test = train_test_split(df, test_size=0.2, random_state=1)"
      ],
      "metadata": {
        "id": "_-371SVUX8to"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "numerical = ['tenure', 'monthlycharges', 'totalcharges']\n",
        "\n",
        "categorical = [\n",
        "    'gender',\n",
        "    'seniorcitizen',\n",
        "    'partner',\n",
        "    'dependents',\n",
        "    'phoneservice',\n",
        "    'multiplelines',\n",
        "    'internetservice',\n",
        "    'onlinesecurity',\n",
        "    'onlinebackup',\n",
        "    'deviceprotection',\n",
        "    'techsupport',\n",
        "    'streamingtv',\n",
        "    'streamingmovies',\n",
        "    'contract',\n",
        "    'paperlessbilling',\n",
        "    'paymentmethod',\n",
        "]"
      ],
      "metadata": {
        "id": "g60ZvTNbYBrl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(df_train, y_train, C=1.0):\n",
        "    dicts = df_train[categorical + numerical].to_dict(orient='records')\n",
        "\n",
        "    dv = DictVectorizer(sparse=False)\n",
        "    X_train = dv.fit_transform(dicts)\n",
        "\n",
        "    model = LogisticRegression(C=C, max_iter=1000)\n",
        "    model.fit(X_train, y_train)\n",
        "    \n",
        "    return dv, model"
      ],
      "metadata": {
        "id": "x9g3DABzYIEK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(df, dv, model):\n",
        "    dicts = df[categorical + numerical].to_dict(orient='records')\n",
        "\n",
        "    X = dv.transform(dicts)\n",
        "    y_pred = model.predict_proba(X)[:, 1]\n",
        "\n",
        "    return y_pred"
      ],
      "metadata": {
        "id": "5Xz3dxcTYMs-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "C = 1.0\n",
        "n_splits = 5"
      ],
      "metadata": {
        "id": "UdJ_YIklYSNg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "kfold = KFold(n_splits=n_splits, shuffle=True, random_state=1)\n",
        "\n",
        "scores = []\n",
        "\n",
        "for train_idx, val_idx in kfold.split(df_full_train):\n",
        "    df_train = df_full_train.iloc[train_idx]\n",
        "    df_val = df_full_train.iloc[val_idx]\n",
        "\n",
        "    y_train = df_train.churn.values\n",
        "    y_val = df_val.churn.values\n",
        "\n",
        "    dv, model = train(df_train, y_train, C=C)\n",
        "    y_pred = predict(df_val, dv, model)\n",
        "\n",
        "    auc = roc_auc_score(y_val, y_pred)\n",
        "    scores.append(auc)\n",
        "\n",
        "print('C=%s %.3f +- %.3f' % (C, np.mean(scores), np.std(scores)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "elAcsflxYWjQ",
        "outputId": "36b29aa3-ba1d-4d83-c599-c3551d22d802"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "C=1.0 0.840 +- 0.008\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "scores"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kFEnQl7FYcEy",
        "outputId": "715b865b-0d1a-4402-b182-1d0f5a3b3272"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.8419943324096679,\n",
              " 0.8455854357038802,\n",
              " 0.8311739915713425,\n",
              " 0.8301684306452645,\n",
              " 0.851750023532365]"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dv, model = train(df_full_train, df_full_train.churn.values, C=1.0)\n",
        "y_pred = predict(df_test, dv, model)\n",
        "\n",
        "auc = roc_auc_score(y_test, y_pred)\n",
        "auc"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 222
        },
        "id": "EfGa3dErYhhS",
        "outputId": "be760166-3018-425e-fe5e-5ed132aa8fb5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-00643723b8c9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mauc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mroc_auc_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mauc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'y_test' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dv, model = train(df_full_train, df_full_train.churn.values, C=1.0)\n",
        "y_pred = predict(df_test, dv, model)\n",
        "\n",
        "y_test = df_test.churn.values\n",
        "auc = roc_auc_score(y_test, y_pred)\n",
        "auc"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X4b3Z2amFprF",
        "outputId": "b18d290e-54f9-4fcf-bcaa-d7c878e9c15e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8572386167896259"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **5.2 Saving and loading the model**(video 2)\n",
        "\n",
        "In this session we'll cover the idea \"How to use the model in future without training and evaluating the code\"\n",
        "\n",
        "  * To save the model we made before there is an option using the pickle library:\n",
        "    * First install the library with the command pip install pickle-mixin if you don't have it.\n",
        "    * After training the model and being the model ready for prediction process uses this code to save the model for later.\n",
        "\n",
        "\n",
        "   "
      ],
      "metadata": {
        "id": "ss6kRt050Ohb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Save the model**"
      ],
      "metadata": {
        "id": "LYcFFb96Yq7V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* For saving the model we will use pickle which is a built-in library for saving python objects."
      ],
      "metadata": {
        "id": "TsCGsUnWIx0Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle"
      ],
      "metadata": {
        "id": "iYh_XPykYmS0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Let's import it, import pickle, take our model and write it to your file, we need to first create a file where we will write it, let's call it **\"output_file = f'model_C={C}.bin\"**"
      ],
      "metadata": {
        "id": "zapAh0IgJFq1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "output_file = f'model_C={C}.bin'\n",
        "output_file "
      ],
      "metadata": {
        "id": "h2ZZIYmfYwM1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "f2520c08-394c-43e2-8cfa-09656fdab330"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'model_C=1.0.bin'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Let's create a special variable for storing the name of the output file, model underscores C, we want to know what is  the C, we use for training this model and put it on file name then bin."
      ],
      "metadata": {
        "id": "pgVQj-kKIKam"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "with open('model.bin', 'wb') as f_out:\n",
        "   pickle.dump((dv, model), f_out)\n",
        "f_out.close() ## After opening any file it's nessecery to close it"
      ],
      "metadata": {
        "id": "t6NnuCv-0i9R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "  * In the code above we'll making a binary file named model.bin and writing the dict_vectorizer for one hot encoding and model as array in it. (We will save it as binary in case it wouldn't be readable by humans)\n",
        "  * To be able to use the model in future without running the code, We need to open the binary file we saved before.\n"
      ],
      "metadata": {
        "id": "qNZMJk9x4kW6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "f_out = open(output_file, 'wb') \n",
        "pickle.dump((dv, model), f_out)\n",
        "f_out.close()## After opening any file it's nessecery to close it"
      ],
      "metadata": {
        "id": "3S9r3luPY4mB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -lh *.bin"
      ],
      "metadata": {
        "id": "mVRhbDo0Y9Xr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4cedde1c-1ee4-4cba-db1a-0e3c0d791b96"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-rw-r--r-- 1 root root 2.8K Oct  4 18:56 'model_C=1.0.bin'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open(output_file, 'wb') as f_out: \n",
        "    pickle.dump((dv, model), f_out)\n",
        "    # do stuff\n",
        "\n",
        "# other stuf"
      ],
      "metadata": {
        "id": "Gu1CHR4fZBnC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "notes:\n",
        "\n",
        "* Take this output file and we want to create a file with, write open function, opens a file and here we going to specify what we want to do with this file, the file to be binary.\n",
        "* Use pickle to save our model, we use the dump function for that, write model and write the file **f_out** and then the last thing, we need to close the file and here I saved on the model but we have actually, two things here we have the dict vectorizer as well, look our predict function we need to have both not just the model because with just the model will not be able to translate a user or a customer into a feature matrix.\n",
        "* We need to have both, write a tuple, we will write to this model underscore C file we will write two things we will write the dictionary vectorizer and write the model.\n",
        "* Open the file and save the model there and we close it calling the close method s pretty important because it, we don't do this we cannot be sure if this file has the content and other service can use it.\n",
        "* It's very easy to accidentally forget to close the file that's why I prefer to use with a statement that makes sure that the file is closed all the time, it automatically closes the file.\n",
        "* We can do it with open output file as f out which is equivalent and put this line **\"pickle.dump(dv,mode),f_out)\"** then do this thing inside the statement, do stuff.\n",
        "* Once we leave this with block do other stuff so everything, do here inside the statement the file is still open once we go outside of the \"with\" statement once, then the file is authomatically lost this is a nice way and easy way to make sure that no matter what you do the file is close to the end and it's also a bit shoter just two lines versus three and this is how we save the models."
      ],
      "metadata": {
        "id": "taAO49VMKlQV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Let me start the kernel so we're pretending we're a different process\n",
        "* just click the kernel to restart kernel [1]"
      ],
      "metadata": {
        "id": "j5cNVcngVty4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load the model\n",
        "\n"
      ],
      "metadata": {
        "id": "WFPPQftHZIYT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle"
      ],
      "metadata": {
        "id": "TgjQyQLZZFop"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Import pickle and you see this is now cell number one [1] because it **started the jupyter kernel** or in **google collab we call it the runtime**.\n",
        "* From scratch it doesn't have to see if we write model here it says, because we restart the kernel and it doesn't have access to the variable we used here, we are starting from a clean slate.\n"
      ],
      "metadata": {
        "id": "ad2Vn8N5Z1uc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_file = 'model_C=1.0.bin'"
      ],
      "metadata": {
        "id": "Q0yDmdgrZPs8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(input_file, 'rb') as f_in: \n",
        "    dv, model = pickle.load(f_in)"
      ],
      "metadata": {
        "id": "djIIl4SBZUlC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* We want to load the model and its' pretty similar to saving the model except here, let's call if **f_in** file input and here I call the f_out which means file output, it's a file that I created for writing and it will be a file for reading and like we need to replace the \"w\" to \"r so we read a file and this is very important if you don't if  you forget accidentally to change it and it leaves us \"w\" here is right there it will overwrite the file so it will just create a new file with zero bytes we don't wan to have that, we want to open this file for reading adn instead of dump we use load, load reads from the file, we have file in and it returns the thing we saved, this is what we saved and this is what we load.\n"
      ],
      "metadata": {
        "id": "FexeGckFbHT4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# with open('mode.bin', 'rb') as f_in:  ## Note that never open a binary file you do not trust!\n",
        "    dict_vectorizer, model = pickle.load(f_in)\n",
        "f_in.close()"
      ],
      "metadata": {
        "id": "W6p5xkbC5Igg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* With unpacking the model and the dict_vectorizer, We're able to again predict new input values without training a new model by re-running the code.\n"
      ],
      "metadata": {
        "id": "C6hU50UB5OMj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dv,model"
      ],
      "metadata": {
        "id": "1jxF26bYZY8a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "66002a87-8c78-4734-dcad-c464ff35afed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(DictVectorizer(sparse=False), LogisticRegression(max_iter=1000))"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Create a variable I'll call it an input file, now we have our dict vectorizer and we have our logistic regression this is what we trained and saved previously and you can also notice that there are no imports, so I don't import sklearn here out, we have to scikit-learn installed on our computer so without that this is will not work, when it will try to load the pickle file it will complain \"saving that\" I don't know what this is I cannot create these classes I cannot create this dictionary vectorizer and logistic regression because scikit-learn is not installed on your computer.\n",
        "* Pretending this person who we want to scores who want to send to our churn service to understand which the probability, the probability of joining\n"
      ],
      "metadata": {
        "id": "iCEdKp9KZr9j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "customer = {\n",
        "    'gender': 'female',\n",
        "    'seniorcitizen': 0,\n",
        "    'partner': 'yes',\n",
        "    'dependents': 'no',\n",
        "    'phoneservice': 'no',\n",
        "    'multiplelines': 'no_phone_service',\n",
        "    'internetservice': 'dsl',\n",
        "    'onlinesecurity': 'no',\n",
        "    'onlinebackup': 'yes',\n",
        "    'deviceprotection': 'no',\n",
        "    'techsupport': 'no',\n",
        "    'streamingtv': 'no',\n",
        "    'streamingmovies': 'no',\n",
        "    'contract': 'month-to-month',\n",
        "    'paperlessbilling': 'yes',\n",
        "    'paymentmethod': 'electronic_check',\n",
        "    'tenure': 1,\n",
        "    'monthlycharges': 29.85,\n",
        "    'totalcharges': 29.85\n",
        "}"
      ],
      "metadata": {
        "id": "KORhFrAFZdKF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dv.transform([customer])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pNHZttBpgsjN",
        "outputId": "94625ce7-3990-45b9-8f3c-41b16ad84e4f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 1.  ,  0.  ,  0.  ,  1.  ,  0.  ,  1.  ,  0.  ,  0.  ,  1.  ,\n",
              "         0.  ,  1.  ,  0.  ,  0.  , 29.85,  0.  ,  1.  ,  0.  ,  0.  ,\n",
              "         0.  ,  1.  ,  1.  ,  0.  ,  0.  ,  0.  ,  1.  ,  0.  ,  1.  ,\n",
              "         0.  ,  0.  ,  1.  ,  0.  ,  1.  ,  0.  ,  0.  ,  1.  ,  0.  ,\n",
              "         0.  ,  1.  ,  0.  ,  0.  ,  1.  ,  0.  ,  0.  ,  1.  , 29.85]])"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* We use our dictionary vectorizer transform method and then put the customer and the dictionary vectorizer expects a list of dictionaries that's why we create a list with just one customer and this is the output, let me save this output into **X** but,"
      ],
      "metadata": {
        "id": "eQhRCdsafW1P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = dv.transform([customer])"
      ],
      "metadata": {
        "id": "YgwKCIQGZjHz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Next well call model predict proper for this **X**, so we have a two-dimensional array as usual and we're interested in this thing here, let's write it like row number zero the column number one and we're getting the probability that this particular customer is going to churn."
      ],
      "metadata": {
        "id": "5s5pHi7Gf2Pp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.predict_proba(X)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eeKvX9xTg8lP",
        "outputId": "186e0209-9fa0-4fb8-e767-4152a39cdac5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.36364158, 0.63635842]])"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.predict_proba(X)[0,1] # if tenure is 1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RcrfSsVyhM5j",
        "outputId": "624f58d7-a1f0-4dfb-f60b-a733a85b6ef7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.6363584152704198"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# if the tenure is 20\n",
        "model.predict_proba(X)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4HysKDdeispi",
        "outputId": "35db230e-6873-4b27-9ca0-1b65571f57e8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.36364158, 0.63635842]])"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.predict_proba(X)[0,1]# if tenure is 20 the  probability is 0.33"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hf5_pyJsh7Ym",
        "outputId": "112295d0-10fe-4304-cd81-ce4eaeac98e0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.3313627488178145"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = model.predict_proba(X)[0, 1]"
      ],
      "metadata": {
        "id": "XgOIH8FwZnSF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('input:', customer)\n",
        "print('output:', y_pred)"
      ],
      "metadata": {
        "id": "Y-4GoRfXZrIA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bb9f5572-019d-4855-f525-da44b5f2ef5b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input: {'gender': 'female', 'seniorcitizen': 0, 'partner': 'yes', 'dependents': 'no', 'phoneservice': 'no', 'multiplelines': 'no_phone_service', 'internetservice': 'dsl', 'onlinesecurity': 'no', 'onlinebackup': 'yes', 'deviceprotection': 'no', 'techsupport': 'no', 'streamingtv': 'no', 'streamingmovies': 'no', 'contract': 'month-to-month', 'paperlessbilling': 'yes', 'paymentmethod': 'electronic_check', 'tenure': 1, 'monthlycharges': 29.85, 'totalcharges': 29.85}\n",
            "output: 0.6363584152704198\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Let's create such a script, a script that trains a model for that what we can do is we can just go to our jupyter and then download this notebook as a python.py file click on that, and it saves the file."
      ],
      "metadata": {
        "id": "ld2XMDdajj2P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Open it in Visual Studio Code."
      ],
      "metadata": {
        "id": "QpGtIZhJkXJ8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Making requests"
      ],
      "metadata": {
        "id": "XswppcJrZzol"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests"
      ],
      "metadata": {
        "id": "3Qoi4nhYZwcf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "url = 'http://localhost:9696/predict'"
      ],
      "metadata": {
        "id": "K4gShxLqZ5ai"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "customer = {\n",
        "    'gender': 'female',\n",
        "    'seniorcitizen': 0,\n",
        "    'partner': 'yes',\n",
        "    'dependents': 'no',\n",
        "    'phoneservice': 'no',\n",
        "    'multiplelines': 'no_phone_service',\n",
        "    'internetservice': 'dsl',\n",
        "    'onlinesecurity': 'no',\n",
        "    'onlinebackup': 'yes',\n",
        "    'deviceprotection': 'no',\n",
        "    'techsupport': 'no',\n",
        "    'streamingtv': 'no',\n",
        "    'streamingmovies': 'no',\n",
        "    'contract': 'two_year',\n",
        "    'paperlessbilling': 'yes',\n",
        "    'paymentmethod': 'electronic_check',\n",
        "    'tenure': 1,\n",
        "    'monthlycharges': 29.85,\n",
        "    'totalcharges': 29.85\n",
        "}"
      ],
      "metadata": {
        "id": "StTXLt1oZ-Us"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = requests.post(url,json=customer).json()"
      ],
      "metadata": {
        "id": "dFvF7i1haE8c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response"
      ],
      "metadata": {
        "id": "bATHVKTIaJWc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if response['churn']:\n",
        "    print('sending email to', 'asdx-123d')"
      ],
      "metadata": {
        "id": "PncZx3eRaNst"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **5.3 Web services: introduction to Flask**(video 3)\n"
      ],
      "metadata": {
        "id": "pmanA98WxAN2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Links**\n",
        "\n",
        "* 0.0.0.0 vs localhost: https://stackoverflow.com/a/20778887/861423"
      ],
      "metadata": {
        "id": "iCHaJSsdw3c6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this session we talked about what is a web service and how to create a simple web service.\n",
        "\n",
        "* What is actually a web service\n",
        "    * A web service is a method used to communicate between electronic devices.\n",
        "    * There are some methods in web services we can use it to satisfy our problems. Here below we would list some.\n",
        "      * GET: GET is a method used to retrieve files, For example whene we are searching for a cat image in google we are actually requesting cat images with GET method.\n",
        "      * POST: POST is the second common method used in web services. For example in a sign up process, when we are submiting our name, username, passwords, etc we are posting our data to a server that is using the web service. (Note that there is no specification where the data goes)\n",
        "      * PUT: PUT is same as POST but we are specifying where the data is going to.\n",
        "      * DELETE: DELETE is a method that is used to request to delete some data from the server.\n",
        "For more information just google the HTTP methods, You'll find useful information about this."
      ],
      "metadata": {
        "id": "BbjOabqx53Jv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* To create a simple web service, there are plenty libraries available in every language. Here we would like to introduce Flask library in python.\n",
        "    * If you haven't installed the library just try installing it with the code pip install Flask\n",
        "    * To create a simple web service just run the code below:"
      ],
      "metadata": {
        "id": "RTOkH4Ai6fqI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from flask import Flask\n",
        "app = Flask('churn-app') # give an identity to your web service\n",
        "@app.route('/ping',methods=[GET])\n",
        "def ping():\n",
        "    return 'PONG'\n",
        "\n",
        "if __name__=='__main__':\n",
        "   app.run('debug=True, host='0.0.0.0', port=9696) # run the code in local machine with the debugging mode true and port 9696"
      ],
      "metadata": {
        "id": "zQHRGSZi6arV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* \n",
        "  * With the code above we made a simple web server and created a route named ping that would send pong string.\n",
        "\n",
        "  * To test it just open your browser and search localhost:9696/ping, You'll see that the 'PONG' string is received. Congrats You've made a simple web server 🥳.\n",
        "\n",
        "* To use our web server to predict new values we must modify it. See how in the next session.\n"
      ],
      "metadata": {
        "id": "WPtdRMBI60uy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **5.4 Serving the churn model with Flask**(video 4)\n",
        "\n",
        "In this session we talked about implementing the functionality of prediction to our churn web service and how to make it usable in development environment.\n",
        "\n",
        "* To make the web service predict the churn value for each customer we must modify the code in session 3 with the code we had in previous chapters. Below we can see how the code works in order to predict the churn value.\n",
        "* In order to predict we need to first load the previous saved model and use a prediction function in a special route.\n",
        "    * To load the previous saved model we use the code below:\n"
      ],
      "metadata": {
        "id": "mrWJrJzC_6pg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "with open('churn-model.bin', 'rb') as f_in:\n",
        "  dv, model = pickle.load(f_in)"
      ],
      "metadata": {
        "id": "2ItSPBTRAPfq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* \n",
        "    * As we had earlier to predict a value for a customer we need a function like below:\n"
      ],
      "metadata": {
        "id": "2CuORlWTAYoN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_single(customer, dv, model):\n",
        "  X = dv.transform([customer])  ## apply the one-hot encoding feature to the customer data \n",
        "  y_pred = model.predict_proba(X)[:, 1]\n",
        "  return y_pred[0]"
      ],
      "metadata": {
        "id": "Vbve4m-nAU7m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* \n",
        "    * Then at last we make the final function used for creating the web service."
      ],
      "metadata": {
        "id": "4YhGoqsuAyDl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@app.route('/predict', methods=['POST'])  ## in order to send the customer information we need to post its data.\n",
        "def predict():\n",
        "customer = request.get_json()  ## web services work best with json frame, So after the user post its data in json format we need to access the body of json.\n",
        "\n",
        "prediction = predict_single(customer, dv, model)\n",
        "churn = prediction >= 0.5\n",
        "\n",
        "result = {\n",
        "    'churn_probability': float(prediction), ## we need to conver numpy data into python data in flask framework\n",
        "    'churn': bool(churn),  ## same as the line above, converting the data using bool method\n",
        "}\n",
        "\n",
        "return jsonify(result)  ## send back the data in json format to the user"
      ],
      "metadata": {
        "id": "MwL_NcWJAtEn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* \n",
        "    * The whole code above is available in this link: https://github.com/alexeygrigorev/mlbookcamp-code/blob/master/chapter-05-deployment/churn_serving.py \n",
        "    * At last run your code. To see the result can't use a simple request in a web browser. We can run the code below to post new user data and see the response.\n"
      ],
      "metadata": {
        "id": "_g-ltDb0A_Ax"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## a new customer informations\n",
        "customer = {\n",
        "  'customerid': '8879-zkjof',\n",
        "  'gender': 'female',\n",
        "  'seniorcitizen': 0,\n",
        "  'partner': 'no',\n",
        "  'dependents': 'no',\n",
        "  'tenure': 41,\n",
        "  'phoneservice': 'yes',\n",
        "  'multiplelines': 'no',\n",
        "  'internetservice': 'dsl',\n",
        "  'onlinesecurity': 'yes',\n",
        "  'onlinebackup': 'no',\n",
        "  'deviceprotection': 'yes',\n",
        "  'techsupport': 'yes',\n",
        "  'streamingtv': 'yes',\n",
        "  'streamingmovies': 'yes',\n",
        "  'contract': 'one_year',\n",
        "  'paperlessbilling': 'yes',\n",
        "  'paymentmethod': 'bank_transfer_(automatic)',\n",
        "  'monthlycharges': 79.85,\n",
        "  'totalcharges': 3320.75\n",
        "}\n",
        "import requests ## to use the POST method we use a library named requests\n",
        "url = 'http://localhost:9696/predict' ## this is the route we made for prediction\n",
        "response = requests.post(url, json=customer) ## post the customer information in json format\n",
        "result = response.json() ## get the server response\n",
        "print(result)"
      ],
      "metadata": {
        "id": "CaH4asuNA7Ne"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Until here we saw how we made a simple web server that predicts the churn value for every user. When you run your app you will see a warning that it is not a WGSI server and not suitable for production environmnets. To fix this issue and run this as a production server there are plenty of ways available.\n",
        "    * One way to create a WSGI server is to use gunicorn. To install it use the command pip install gunicorn, And to run the WGSI server you can simply run it with the command gunicorn --bind 0.0.0.0:9696 churn:app. Note that in churn:app the name churn is the name we set for our the file containing the code app = Flask('churn')(for example: churn.py), You may need to change it to whatever you named your Flask app file.\n",
        "    * Windows users may not be able to use gunicorn library because windows system do not support some dependecies of the library. So to be able to run this on a windows machine, there is an alternative library waitress and to install it just use the command pip install waitress.\n",
        "    * to run the waitress wgsi server use the command waitress-serve --listen=0.0.0.0:9696 churn:app.\n",
        "To test it just you can run the code above and the results is the same.\n",
        "* So until here you were able to make a production server that predicts the churn value for new customers. In the next session, we can see how to solve library version conflicts in each machine and manage the dependencies for production environments.\n"
      ],
      "metadata": {
        "id": "qaMnKOoBBZHk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **5.5 Python virtual environment: Pipenv** (video 5)\n",
        "\n",
        "In this session we're going to make virtual environment for our project. So Let's start this session to get to know what is a virtual environment and how to make it.\n",
        "\n",
        "* Every time we're running a file from a directory we're using the executive files from a global directory. For example when we install python on our machine the executable files that are able to run our codes will go to somewhere like /home/username/python/bin/ for example the pip command may go to /home/username/python/bin/pip.\n",
        "* Sometimes the versions of libraries conflict (the project may not run or get into massive errors). For example, we have an old project that uses the sklearn library with version 0.24.1 and now we want to run it using sklearn version 1.0.0. We may get into errors because of the version conflict.\n",
        "    * To solve the conflict we can make virtual environments. The virtual environment is something that can separate the libraries installed in our system and the libraries with the specified version we want our project to run with. There are a lot of ways to create a virtual environments. One way we are going to use is using a library named pipenv.\n",
        "    * pipenv is a library that can create a virutal environment. To install this library just use the classic method pip install pipenv.\n",
        "    * After installing pipenv we must to install the libraries we want for our project in the new virtual environment. It's really easy, Just use the command pipenv instead of pip. pipenv install numpy sklearn==0.24.1 flask. With this command we installed the libraries we want for our project.\n",
        "    * Note that using the pipenv command we made two files named Pipfile and Pipfile.lock. If we look at this files closely we can see that in Pipfile the libraries we installed are named. If we specified the library name, it's also specified in Pipfile.\n",
        "    * In Pipfile.lock we can see that each library with it's installed version is named and a hash file is there to reproduce if we move the environment to another machine.\n",
        "    * If we want to run the project in another machine, we can easily installed the libraries we want with the command pipenv install. This command will look into Pipfile and Pipfile.lock to install the libraries with specified version.\n",
        "    * After installing the required libraries we can run the project in the virtual environment with the pipenv shell command. This will go to the virtual environment's shell and then any command we execute will use the virtual environment's libraries.\n",
        "* Installing and using the libraries such as gunicorn is the same as the last session.\n",
        "* Until here we made a virtual environment for our libraries with a required specified version. To separate this environment more, such as making unicorn be able to run on windows machines we need another way. The other way is using Docker. Docker allows us to separate everything more than before and makes any project able to run on any machine that supports Docker smoothly.\n",
        "* In the next session, we'll go into detail about how Docker works and how to use it."
      ],
      "metadata": {
        "id": "3x8l5uTlCNHY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **5.6 Environment management: Docker**(video 6)\n",
        "\n",
        "## **Installing Docker**\n",
        "____\n",
        "To isolate more our project file from our system machine, there is an option named Docker. With Docker you are able to pack all your project is a system that you want and run it in any system machine. For example if you want Ubuntu 20.4 you can have it in a mac or windows machine or other operating systems.\n",
        "To get started with Docker for the churn prediction project you can follow the instructions below.\n",
        "\n"
      ],
      "metadata": {
        "id": "JE1yzC2eD0hc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Ubuntu**\n",
        "\n",
        "* To install Docker run the command below.\n",
        "\n"
      ],
      "metadata": {
        "id": "FmGFOlwREZtH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bash\n",
        "sudo apt-get install docker.io"
      ],
      "metadata": {
        "id": "lYeiMnYmCHPW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* To run docker without sudo, follow this instruction:https://docs.docker.com/engine/install/linux-postinstall/.\n",
        "    * Once our project was packed in a Docker container, we're able to run our project on any machine.\n",
        "    * First we have to make a Docker image. In Docker image file there are settings and dependecies we have in our project. To find Docker images that you need you can simply search the Docker website: https://hub.docker.com/search?type=image.\n",
        "    * Here a Docker file is written we'll explain it below.(There should be no comments in Docker file, So remove the comments if you want to copy it)"
      ],
      "metadata": {
        "id": "xYPseXx6Ep2k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "FROM python:3.8.12-slim                                                     # First install the python 3.8, the slim version have less size\n",
        "RUN pip install pipenv                                                      # Install pipenv library in Docker \n",
        "WORKDIR /app                                                                # we have created a directory in Docker named app and we're using it as work directory \n",
        "COPY [\"Pipfile\", \"Pipfile.lock\", \"./\"]                                      # Copy the Pip files into our working derectory \n",
        "RUN pipenv install --deploy --system                                        # install the pipenv dependecies we had from the project and deploy them \n",
        "COPY [\"*.py\", \"churn-model.bin\", \"./\"]                                      # Copy any python files and the model we had to the working directory of Docker \n",
        "EXPOSE 9696                                                                 # We need to expose the 9696 port because we're not able to communicate with Docker outside it\n",
        "ENTRYPOINT [\"gunicorn\", \"--bind\", \"0.0.0.0:9696\", \"churn_serving:app\"]      # If we run the Docker image, we want our churn app to be running"
      ],
      "metadata": {
        "id": "C6afCjBaEk7m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* If we don't put the last line ENTRYPOINT, we will be in a python shell. Note that in Docker we made put in double quotes, This is because of the spaces. We have to ignore spaces in a command and put the characters in double quotes.(See ENTRYPOINT for example)\n",
        "    * After creating the Dockerfile and writing the settings we want in it, We need to build it with the command below."
      ],
      "metadata": {
        "id": "V9gS2I8tFept"
      }
    },
    {
      "cell_type": "code",
      "source": [
        " docker build -t churn-prediction ."
      ],
      "metadata": {
        "id": "8Zmvzj6eFZJk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* With -t command We're specifying the name churn-prediction for this Dockerfile.\n",
        "    * To run it, Simply execute the command below:"
      ],
      "metadata": {
        "id": "58BowfEmFs_E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        " docker run -it -p 9696:9696 churn-prediction:latest"
      ],
      "metadata": {
        "id": "v5y4BIvjFpV1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Here we use the option -it in order to the Docker run from terminal and shows the result. The -p parameter is used to map the 9696 port of the Docker to 9696 port of our machine.(first 9696 is the port number of our machine and the last one is Docker container port.)\n",
        "    * At last you've deployed your prediction app inside a Docker continer. Congratulations 🥳\n"
      ],
      "metadata": {
        "id": "UIqrMxU2F23j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Windows**\n",
        "\n",
        "To install the Docker you can just follow the instruction by Andrew Lock in this link: https://andrewlock.net/installing-docker-desktop-for-windows/"
      ],
      "metadata": {
        "id": "Z2nvqi0gGHkI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **5.7 Deployment to the cloud: AWS Elastic Beanstalk (optional)**\n",
        "\n",
        "## **Links**\n",
        "____\n",
        "* Creating an account on AWS:https://mlbookcamp.com/article/aws\n",
        "\n"
      ],
      "metadata": {
        "id": "lUyZVaaEGM0F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Notes**\n",
        "___\n",
        "As we see how to deploy our apps in AWS Let's find it out how to deploy them in Heroku.\n",
        "\n"
      ],
      "metadata": {
        "id": "3k8reOU-IDOT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Heroku**\n",
        "\n",
        "Here we will learn how to deploy our apps in Heroku instead of AWS.\n",
        "\n",
        "* First of all create your web service with flask. (example file: churn_prediction.py\n",
        "* Then create a file named requirements.txt and assing your dependencies there. Example:\n"
      ],
      "metadata": {
        "id": "3cs6v6NkINNU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pickle\n",
        "numpy\n",
        "flask\n",
        "gunicorn"
      ],
      "metadata": {
        "id": "9_NVx05yGBNk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Create another file named Procfile and add the app you want to be able to run there. Example:\n"
      ],
      "metadata": {
        "id": "VqqeH674IrGu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "web: gunicorn churn_serving:app"
      ],
      "metadata": {
        "id": "x9s-ns2HImEc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note that the churn_serving name in the box above is the name of the main python file we're going to be runned.\n",
        "\n",
        "* Create your heroku profile, Go to dashboard and the Deploy tab.\n",
        "* Follow the instruction for Deploy using Heroku Git.\n",
        "* Great your app is now available from global universe.\n",
        "\n",
        "I've put my heroku app files in this repository: https://github.com/amindadgar/customer-churn-app\n",
        "\n"
      ],
      "metadata": {
        "id": "s0Y3ZbfvI6yp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **5.8 Summary**\n",
        "\n",
        "## Notes\n",
        "____\n",
        "In this chapter we learned this topics:\n",
        "\n",
        "We learned how to save the model and load it to re-use it without running the previous code.\n",
        "* How to deploy the model in a web service.\n",
        "* How to create a virtual environment.\n",
        "* How to create a container and run our code in any operating system.\n",
        "* How to implement our code in a public web service and access it from outside a local computer.\n",
        "In the next chapter, we would learn algorithms such as Decision trees, Random forests, and Gradient boosting as an alternative way of combining decision trees."
      ],
      "metadata": {
        "id": "VMYRcod-JetV"
      }
    }
  ]
}